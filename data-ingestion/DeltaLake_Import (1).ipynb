{
 "metadata": {
  "kernelspec": {
   "name": "glue_pyspark",
   "display_name": "Glue PySpark",
   "language": "python"
  },
  "language_info": {
   "name": "Python_Glue_Session",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "pygments_lexer": "python3",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "%session_id_prefix native-delta-dataframe-\n",
    "%glue_version 3.0\n",
    "%idle_timeout 60\n",
    "%%configure\n",
    "{\n",
    "  \"--conf\": \"spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
    "  \"--datalake-formats\": \"delta\"\n",
    "}"
   ],
   "metadata": {
    "trusted": true,
    "tags": []
   },
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.4 \nSetting session ID prefix to native-delta-dataframe-\nSetting Glue version to: 3.0\nCurrent idle_timeout is None minutes.\nidle_timeout has been set to 60 minutes.\nThe following configurations have been updated: {'--conf': 'spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog', '--datalake-formats': 'delta'}\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create database"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "target_bucket = \"lakehouse-deltalake\"\n",
    "source_bucket = \"tpc-h-dataset\"\n",
    "tpc_h_scale_factor = 1\n",
    "tpc_h_database_name = f\"tpc_h_sf{tpc_h_scale_factor}\"\n",
    "tpc_h_tables = [\"region\", \"nation\", \"customer\", \"lineitem\", \"orders\", \"part\", \"partsupp\", \"supplier\"]\n",
    "\n",
    "target_bucket_prefix = f\"{target_bucket}/{tpc_h_database_name}/\"\n",
    "database_location = f\"s3://{target_bucket_prefix}\""
   ],
   "metadata": {
    "trusted": true,
    "tags": []
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "text": "Trying to create a Glue session for the kernel.\nSession Type: glueetl\nWorker Type: G.1X\nNumber of Workers: 5\nSession ID: add91ad0-e394-4ba0-bc61-054fba98e207\nApplying the following default arguments:\n--glue_kernel_version 1.0.4\n--enable-glue-datacatalog true\n--conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog\n--datalake-formats delta\nWaiting for session add91ad0-e394-4ba0-bc61-054fba98e207 to get into ready status...\nSession add91ad0-e394-4ba0-bc61-054fba98e207 has been created.\n\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import boto3\n",
    "import time\n",
    "\n",
    "## Delete existing Files in DeltaLake S3 Bucket\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket(target_bucket)\n",
    "bucket.objects.filter(Prefix=target_bucket_prefix).delete()\n",
    "\n",
    "## Drop Tables in Glue Data Catalog\n",
    "for table_name in tpc_h_tables:\n",
    "    try:\n",
    "        glue = boto3.client('glue')\n",
    "        glue.delete_table(DatabaseName=tpc_h_database_name, Name=table_name)\n",
    "    except glue.exceptions.EntityNotFoundException:\n",
    "        print(f\"Table {tpc_h_database_name}.{table_name} does not exist\")\n"
   ],
   "metadata": {
    "trusted": true,
    "tags": []
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "text": "Table tpc_h_sf1.region does not exist\nTable tpc_h_sf1.nation does not exist\nTable tpc_h_sf1.customer does not exist\nTable tpc_h_sf1.lineitem does not exist\nTable tpc_h_sf1.orders does not exist\nTable tpc_h_sf1.part does not exist\nTable tpc_h_sf1.partsupp does not exist\nTable tpc_h_sf1.supplier does not exist\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "try:\n",
    "    glue = boto3.client('glue')\n",
    "    res = glue.get_database(Name=tpc_h_database_name)\n",
    "    print(f\"Database {tpc_h_database_name} exists.\")\n",
    "    if 'LocationUri' not in res['Database']:\n",
    "        print(f\"Warning: Database {tpc_h_database_name} does not have Location. You need to configure location in the database.\")\n",
    "except glue.exceptions.EntityNotFoundException:\n",
    "    print(f\"Database {tpc_h_database_name} does not exist.\")\n",
    "    glue = glue.create_database(\n",
    "        DatabaseInput={\n",
    "            'Name': tpc_h_database_name,\n",
    "            'LocationUri': database_location\n",
    "        }\n",
    "    )\n",
    "    print(f\"Created a new database {tpc_h_database_name}.\")"
   ],
   "metadata": {
    "trusted": true,
    "tags": []
   },
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "text": "Database tpc_h_sf1 does not exist.\nCreated a new database tpc_h_sf1.\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create tables"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "start = time.time()"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "query = f\"\"\"\n",
    "CREATE TABLE {tpc_h_database_name}.region\n",
    "(\n",
    "    R_REGIONKEY INT,\n",
    "    R_NAME STRING,\n",
    "    R_COMMENT STRING\n",
    ") USING delta LOCATION '{database_location}region';\n",
    "\"\"\"\n",
    "spark.sql(query)"
   ],
   "metadata": {
    "trusted": true,
    "tags": []
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": "DataFrame[]\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "query = f\"\"\"\n",
    "CREATE TABLE {tpc_h_database_name}.nation\n",
    "(\n",
    "    N_NATIONKEY INT,\n",
    "    N_NAME STRING,\n",
    "    N_REGIONKEY INT,\n",
    "    N_COMMENT STRING\n",
    ") USING delta LOCATION '{database_location}nation';\n",
    "\"\"\"\n",
    "spark.sql(query)"
   ],
   "metadata": {
    "trusted": true,
    "tags": []
   },
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "text": "DataFrame[]\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "query = f\"\"\"\n",
    "CREATE TABLE {tpc_h_database_name}.customer\n",
    "(\n",
    "    C_CUSTKEY INT,\n",
    "    C_NAME STRING,\n",
    "    C_ADDRESS STRING,\n",
    "    C_NATIONKEY INT,\n",
    "    C_PHONE STRING,\n",
    "    C_ACCTBAL DOUBLE,\n",
    "    C_MKTSEGMENT STRING,\n",
    "    C_COMMENT STRING\n",
    ") USING delta LOCATION '{database_location}customer';\n",
    "\"\"\"\n",
    "spark.sql(query)"
   ],
   "metadata": {
    "trusted": true,
    "tags": []
   },
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "text": "DataFrame[]\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "query = f\"\"\"\n",
    "CREATE TABLE {tpc_h_database_name}.lineitem\n",
    "(\n",
    "    L_ORDERKEY INT,\n",
    "    L_PARTKEY INT,\n",
    "    L_SUPPKEY INT,\n",
    "    L_LINENUMBER INT,\n",
    "    L_QUANTITY DOUBLE,\n",
    "    L_EXTENDEDPRICE DOUBLE,\n",
    "    L_DISCOUNT DOUBLE,\n",
    "    L_TAX DOUBLE,\n",
    "    L_RETURNFLAG STRING,\n",
    "    L_LINESTATUS STRING,\n",
    "    L_SHIPDATE DATE,\n",
    "    L_COMMITDATE DATE,\n",
    "    L_RECEIPTDATE DATE,\n",
    "    L_SHIPINSTRUCT STRING,\n",
    "    L_SHIPMODE STRING,\n",
    "    L_COMMENT STRING\n",
    ") USING delta LOCATION '{database_location}lineitem';\n",
    "\"\"\"\n",
    "spark.sql(query)"
   ],
   "metadata": {
    "trusted": true,
    "tags": []
   },
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "text": "DataFrame[]\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "query = f\"\"\"\n",
    "CREATE TABLE {tpc_h_database_name}.orders\n",
    "(\n",
    "    O_ORDERKEY INT,\n",
    "    O_CUSTKEY INT,\n",
    "    O_ORDERSTATUS STRING,\n",
    "    O_TOTALPRICE DOUBLE,\n",
    "    O_ORDERDATE DATE,\n",
    "    O_ORDERPRIORITY STRING,\n",
    "    O_CLERK STRING,\n",
    "    O_SHIPPRIORITY INT,\n",
    "    O_COMMENT STRING\n",
    ") USING delta LOCATION '{database_location}orders';\n",
    "\"\"\"\n",
    "spark.sql(query)"
   ],
   "metadata": {
    "trusted": true,
    "tags": []
   },
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "text": "DataFrame[]\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "query = f\"\"\"\n",
    "CREATE TABLE {tpc_h_database_name}.part\n",
    "(\n",
    "    P_PARTKEY INT,\n",
    "    P_NAME STRING,\n",
    "    P_MFGR STRING,\n",
    "    P_BRAND STRING,\n",
    "    P_TYPE STRING,\n",
    "    P_SIZE INT,\n",
    "    P_CONTAINER STRING,\n",
    "    P_RETAILPRICE DOUBLE,\n",
    "    P_COMMENT STRING\n",
    ") USING delta LOCATION '{database_location}part';\n",
    "\"\"\"\n",
    "spark.sql(query)"
   ],
   "metadata": {
    "trusted": true,
    "tags": []
   },
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "text": "DataFrame[]\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "query = f\"\"\"\n",
    "CREATE TABLE {tpc_h_database_name}.partsupp\n",
    "(\n",
    "    PS_PARTKEY INT,\n",
    "    PS_SUPPKEY INT,\n",
    "    PS_AVAILQTY INT,\n",
    "    PS_SUPPLYCOST DOUBLE,\n",
    "    PS_COMMENT STRING\n",
    ") USING delta LOCATION '{database_location}partsupp';\n",
    "\"\"\"\n",
    "spark.sql(query)"
   ],
   "metadata": {
    "trusted": true,
    "tags": []
   },
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "text": "DataFrame[]\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "query = f\"\"\"\n",
    "CREATE TABLE {tpc_h_database_name}.supplier\n",
    "(\n",
    "    S_SUPPKEY INT,\n",
    "    S_NAME STRING,\n",
    "    S_ADDRESS STRING,\n",
    "    S_NATIONKEY INT,\n",
    "    S_PHONE STRING,\n",
    "    S_ACCTBAL DOUBLE,\n",
    "    S_COMMENT STRING\n",
    ") USING delta LOCATION '{database_location}supplier';\n",
    "\"\"\"\n",
    "spark.sql(query)"
   ],
   "metadata": {
    "trusted": true,
    "tags": []
   },
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "text": "DataFrame[]\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "duration = time.time() - start\n",
    "print(f\"Total to create tables: {duration}\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Table Data from S3"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from delta.tables import *\n",
    "\n",
    "for table_name in tpc_h_tables:\n",
    "    print(f\"Importing table {table_name}\")\n",
    "    table_location = f\"{database_location}{table_name}\"\n",
    "    csv_location = f\"s3://{source_bucket}/sf{tpc_h_scale_factor}/{table_name}/\"\n",
    "    \n",
    "    start = time.time()\n",
    "\n",
    "    # Read the schema of the Delta table\n",
    "    delta_table_schema = spark.read.format(\"delta\").load(table_location).schema\n",
    "\n",
    "    duration_1 = time.time() - start\n",
    "    print(f\"Time for reading schema of {table_name}: {duration_1}\")\n",
    "    start_2 = time.time()\n",
    "\n",
    "    # Define the schema for the CSV file based on the Delta table schema\n",
    "    csv_schema = StructType([StructField(field.name, field.dataType, field.nullable) for field in delta_table_schema])\n",
    "\n",
    "    # Read the CSV file into a DataFrame with defined schema\n",
    "    df_orders = spark.read.options(delimiter=\"|\", header=False, inferSchema=True).schema(csv_schema).csv(csv_location)\n",
    "    \n",
    "    duration_2 = time.time() - start_2\n",
    "    print(f\"Time for reading csv {table_name}: {duration_2}\")\n",
    "    start_3 = time.time()\n",
    "\n",
    "    df_orders.write.format(\"delta\").mode(\"overwrite\").option(\"path\", table_location).saveAsTable(f\"{tpc_h_database_name}.{table_name}\")\n",
    "    \n",
    "    duration_3 = time.time() - start_3\n",
    "    print(f\"Time for writing delta table {table_name}: {duration_3}\")\n",
    "    duration = time.time() - start\n",
    "    print(f\"Total time for {table_name}: {duration}\")"
   ],
   "metadata": {
    "trusted": true,
    "tags": []
   },
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "text": "Importing table region\nTime for reading schema of region: 0.08390641212463379\nTime for reading csv region: 0.2007737159729004\nTime for writing delta table region: 4.351106405258179\nTotal time for region: 4.635859727859497\nImporting table nation\nTime for reading schema of nation: 0.0602872371673584\nTime for reading csv nation: 0.1536424160003662\nTime for writing delta table nation: 4.272294521331787\nTotal time for nation: 4.486300468444824\nImporting table customer\nTime for reading schema of customer: 0.06468987464904785\nTime for reading csv customer: 0.1550302505493164\nTime for writing delta table customer: 4.602980375289917\nTotal time for customer: 4.822767019271851\nImporting table lineitem\nTime for reading schema of lineitem: 0.05806779861450195\nTime for reading csv lineitem: 0.17192649841308594\nTime for writing delta table lineitem: 21.426116943359375\nTotal time for lineitem: 21.656177043914795\nImporting table orders\nTime for reading schema of orders: 0.062352657318115234\nTime for reading csv orders: 0.1586611270904541\nTime for writing delta table orders: 8.397846698760986\nTotal time for orders: 8.618925333023071\nImporting table part\nTime for reading schema of part: 0.060935258865356445\nTime for reading csv part: 0.1405181884765625\nTime for writing delta table part: 4.786994695663452\nTotal time for part: 4.988525152206421\nImporting table partsupp\nTime for reading schema of partsupp: 0.06178617477416992\nTime for reading csv partsupp: 0.16173052787780762\nTime for writing delta table partsupp: 5.836082220077515\nTotal time for partsupp: 6.059675931930542\nImporting table supplier\nTime for reading schema of supplier: 0.07697057723999023\nTime for reading csv supplier: 0.1377122402191162\nTime for writing delta table supplier: 4.004020929336548\nTotal time for supplier: 4.218776226043701\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}
